{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8d2ecc4-573f-423c-84e0-c448ce0dc3ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Q1. Derive points table for icc world cup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12e7c59e-7d26-41e7-9b83-98c2e8bd55aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+----+-----+\n| Team|Matches_played|wins|loses|\n+-----+--------------+----+-----+\n|India|             2|   2|    0|\n|   SL|             2|   0|    2|\n|   SA|             1|   0|    1|\n|  Eng|             2|   1|    1|\n|  Aus|             2|   1|    1|\n|   NZ|             1|   1|    0|\n+-----+--------------+----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "columns = ['Team_1', 'Team_2', 'Winner']\n",
    "data = [('India','SL','India'), ('SL','Aus','Aus'), ('SA','Eng','Eng'), ('Eng','NZ','NZ'), ('Aus','India','India') ]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "from pyspark.sql.functions import when, count, sum, col\n",
    "df_1 = df.select(col('Team_1').alias('Team'), when(df.Team_1==df.Winner, 1).otherwise(0).alias('win_flag'))\n",
    "df_2 = df.select(col('Team_2').alias('Team'), when(df.Team_2==df.Winner, 1).otherwise(0).alias('win_flag'))\n",
    "df_final = df_1.union(df_2).groupBy('Team').agg(count('Team').alias('Matches_played'), sum('win_flag').alias('wins')).withColumn('loses', col(\"matches_played\") - col(\"wins\"))  \n",
    "df_final.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a30e0b42-697c-4c8f-9acb-ac99ae462f8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Q2. Find the number of new and repeat customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6969e63-188c-4b7b-8c61-4bec9e73d273",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+-------------+----------------+\n|order_date|total_customers|new_customers|repeat_customers|\n+----------+---------------+-------------+----------------+\n|2022-01-01|              3|            3|               0|\n|2022-01-02|              3|            2|               1|\n|2022-01-03|              3|            1|               2|\n+----------+---------------+-------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "\\\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.sql.functions import col, to_date, min\n",
    "schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"order_date\", StringType(), True),\n",
    "    StructField(\"order_amount\", IntegerType(), True)\n",
    "])\n",
    "data = [\n",
    "    (1, 100, '2022-01-01', 2000),\n",
    "    (2, 200, '2022-01-01', 2500),\n",
    "    (3, 300, '2022-01-01', 2100),\n",
    "    (4, 100, '2022-01-02', 2000),\n",
    "    (5, 400, '2022-01-02', 2200),\n",
    "    (6, 500, '2022-01-02', 2700),\n",
    "    (7, 100, '2022-01-03', 3000),\n",
    "    (8, 400, '2022-01-03', 1000),\n",
    "    (9, 600, '2022-01-03', 3000)\n",
    "]\n",
    "#method1\n",
    "customer_orders_df = spark.createDataFrame(data, schema)\n",
    "Total_customers_df = customer_orders_df.groupBy('order_date').agg(count('*').alias('total_customers'))\n",
    "new_customers_df = customer_orders_df.alias('o1').join(customer_orders_df.alias('o2'), (col('o1.customer_id')==col('o2.customer_id'))&(col('o1.order_date')>col('o2.order_date')), 'left' ).filter(col('o2.order_id').isNull()).select(col('o1.order_date').alias('order_date'))\n",
    "new_customers_df = new_customers_df.groupBy('order_date').agg(count('*').alias('new_customers'))\n",
    "final_df = Total_customers_df.join(new_customers_df, 'order_date', 'fullouter')\n",
    "final_df = final_df.withColumn('repeat_customers', col('total_customers')-col('new_customers'))\n",
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48c2e13d-7aea-40a7-a491-c6c1f7a37216",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+----------------+\n|order_date|new_customers|repeat_customers|\n+----------+-------------+----------------+\n|2022-01-01|            3|               0|\n|2022-01-02|            2|               1|\n|2022-01-03|            1|               2|\n+----------+-------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "#method2\n",
    "from pyspark.sql.functions import col, to_date, min, when\n",
    "customer_orders_df = customer_orders_df.withColumn('order_date', to_date(col('order_date'), 'yyyy-MM-dd'))\n",
    "\n",
    "first_orders_df = customer_orders_df.groupBy('customer_id').agg(min('order_date').alias('first_order_date'))\n",
    "new_df = customer_orders_df.join(first_orders_df, 'customer_id', 'inner').withColumn('new_customers_flg', when(col('order_date')==col('first_order_date'),1).otherwise(0)).withColumn('repeat_customers_flg', when(col('order_date')!=col('first_order_date'),1).otherwise(0))\n",
    "ans_df = new_df. groupBy('order_date').agg(sum('new_customers_flg').alias('new_customers'), sum('repeat_customers_flg').alias('repeat_customers'))\n",
    "ans_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "701b1efb-2972-440e-a810-6fd3b3ffdd62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Q3. scenario based question\n",
    "-- output should be in the below format\n",
    "-- name, total_visits, most visisted floor, resources used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccaaa63e-a9a9-4b2b-aa02-56b9cf1af043",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+------------------+------------------+\n|name|total_visits|most_visited_floor|    resources_used|\n+----+------------+------------------+------------------+\n|   A|           3|                 1|    [DESKTOP, CPU]|\n|   B|           3|                 2|[DESKTOP, MONITOR]|\n+----+------------+------------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "entries_data = [\n",
    "    ('A', 'Bangalore', 'A@gmail.com', 1, 'CPU'),\n",
    "    ('A', 'Bangalore', 'A1@gmail.com', 1, 'CPU'),\n",
    "    ('A', 'Bangalore', 'A2@gmail.com', 2, 'DESKTOP'),\n",
    "    ('B', 'Bangalore', 'B@gmail.com', 2, 'DESKTOP'),\n",
    "    ('B', 'Bangalore', 'B1@gmail.com', 2, 'DESKTOP'),\n",
    "    ('B', 'Bangalore', 'B2@gmail.com', 1, 'MONITOR')\n",
    "]\n",
    "from pyspark.sql.functions import col, count, collect_set, row_number\n",
    "entries_columns = [\"name\", \"address\", \"email\", \"floor\", \"resources\"]\n",
    "entries_df = spark.createDataFrame(entries_data, entries_columns)\n",
    "floors_visits_df = entries_df.groupBy('name', 'floor').agg(count('*').alias('visit_times')).withColumn('rn', row_number().over(Window.partitionBy('name').orderBy(col('visit_times').desc())))\n",
    "total_visits_df = entries_df.groupBy('name').agg(count('name').alias('total_visits'), collect_set('resources').alias('resources_used'))\n",
    "final_df = total_visits_df.join(floors_visits_df, 'name', 'inner').filter(col('rn')==1).select('name','total_visits', col('floor').alias('most_visited_floor'),'resources_used')\n",
    "final_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f8768c2-e01b-4773-ab43-d421ca8dde78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Q4.write pyspark code for finding Date for nth occurence of day in future from given date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "113682f8-f9e0-4777-b201-eb29cc35dfa1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+----------+\n|given_date|nth_occurrence|nth_sunday|\n+----------+--------------+----------+\n|2023-12-21|             3|2024-01-07|\n+----------+--------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import col, lit, expr, date_add\n",
    "from datetime import datetime, timedelta\n",
    "data = [(\"2023-12-21\", 3)]  \n",
    "columns = [\"given_date\", \"nth_occurrence\"]\n",
    "input_df = spark.createDataFrame(data, columns)\n",
    "\n",
    "def get_nth_sunday(given_date: str, nth: int) -> str:\n",
    "    start_date = datetime.strptime(given_date, \"%Y-%m-%d\")\n",
    "    days_to_next_sunday = (6 - start_date.weekday()) % 7  \n",
    "    first_sunday = start_date + timedelta(days=days_to_next_sunday)\n",
    "    nth_sunday = first_sunday + timedelta(weeks=(nth - 1))\n",
    "    return nth_sunday.strftime(\"%Y-%m-%d\")\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "nth_sunday_udf = udf(get_nth_sunday, StringType())\n",
    "result_df = input_df.withColumn(\n",
    "    \"nth_sunday\", nth_sunday_udf(col(\"given_date\"), col(\"nth_occurrence\"))\n",
    ")\n",
    "\n",
    "result_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "520042bd-912b-4951-ac1c-4c66366b28e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Q6. write a  query to find person id, name, number of friends, sum of marks of the person who has friends with total Score\n",
    "-- greater than 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74977f6c-ce34-4257-908b-f301c424b769",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----------+------------+\n|pid|Name|num_friends|person_score|\n+---+----+-----------+------------+\n|  2| Bob|          2|          11|\n|  4|Tara|          3|          45|\n+---+----+-----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "friend_data = [\n",
    "    (1, 2),\n",
    "    (1, 3),\n",
    "    (2, 1),\n",
    "    (2, 3),\n",
    "    (3, 5),\n",
    "    (4, 2),\n",
    "    (4, 3),\n",
    "    (4, 5)\n",
    "]\n",
    "person_data = [\n",
    "    (1, \"Alice\", 88),\n",
    "    (2, \"Bob\", 11),\n",
    "    (3, \"Devis\", 27),\n",
    "    (4, \"Tara\", 45),\n",
    "    (5, \"John\", 63)\n",
    "]\n",
    "from pyspark.sql.functions import col, min, count, sum\n",
    "friend_schema = [\"pid\", \"fid\"]\n",
    "person_schema = [\"PersonID\", \"Name\", \"Score\"]\n",
    "friend_df = spark.createDataFrame(friend_data, schema=friend_schema)\n",
    "person_df = spark.createDataFrame(person_data, schema=person_schema)\n",
    "df = friend_df.join(person_df, friend_df.fid == person_df.PersonID, 'inner').select('pid', 'fid', col('score').alias('f_score'))\n",
    "df = df.join(person_df, df.pid == person_df.PersonID, 'inner').select('pid', 'fid', 'f_score', 'Name', col('Score').alias('P_score'))\n",
    "final_df = df.groupBy('pid').agg(min('Name').alias('Name'), count('fid').alias('num_friends'), sum('f_score').alias('sf_score'), min('p_score').alias('person_score')).filter(col('sf_score')>100).drop(col('sf_score'))\n",
    "final_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f946a06b-4286-4e54-8a30-9340f6cbe4d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Q7.Write a SQL query to find the cancellation rate of requests with unbanned users \n",
    "(both client and driver must not be banned) each day between \"2013-10-01\" and \"2013-10-03\".\n",
    "Round Cancellation Rate to two decimal points.\n",
    "I The cancellation rate is computed by dividing the number of canceled (by client or driver)\n",
    "requests  with unbanned users by the total number of requests with unbanned users on that day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "150ab6a3-79bf-421a-b082-9100633e33b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------+\n|request_at|cancelled_trips_rates|\n+----------+---------------------+\n|2013-10-03|                 50.0|\n|2013-10-01|   33.333333333333336|\n|2013-10-02|                  0.0|\n+----------+---------------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trips_data = [\n",
    "    (1, 1, 10, 1, 'completed', '2013-10-01'),\n",
    "    (2, 2, 11, 1, 'cancelled_by_driver', '2013-10-01'),\n",
    "    (3, 3, 12, 6, 'completed', '2013-10-01'),\n",
    "    (4, 4, 13, 6, 'cancelled_by_client', '2013-10-01'),\n",
    "    (5, 1, 10, 1, 'completed', '2013-10-02'),\n",
    "    (6, 2, 11, 6, 'completed', '2013-10-02'),\n",
    "    (7, 3, 12, 6, 'completed', '2013-10-02'),\n",
    "    (8, 2, 12, 12, 'completed', '2013-10-03'),\n",
    "    (9, 3, 10, 12, 'completed', '2013-10-03'),\n",
    "    (10, 4, 13, 12, 'cancelled_by_driver', '2013-10-03')\n",
    "]\n",
    "trips_columns = [\"id\", \"client_id\", \"driver_id\", \"city_id\", \"status\", \"request_at\"]\n",
    "trips_df = spark.createDataFrame(trips_data, trips_columns)\n",
    "users_data = [\n",
    "    (1, 'No', 'client'),\n",
    "    (2, 'Yes', 'client'),\n",
    "    (3, 'No', 'client'),\n",
    "    (4, 'No', 'client'),\n",
    "    (10, 'No', 'driver'),\n",
    "    (11, 'No', 'driver'),\n",
    "    (12, 'No', 'driver'),\n",
    "    (13, 'No', 'driver')\n",
    "]\n",
    "from pyspark.sql.functions import when\n",
    "users_columns = [\"users_id\", \"banned\", \"role\"]\n",
    "users_df = spark.createDataFrame(users_data, users_columns)\n",
    "uc_df = trips_df.join(users_df, (trips_df.client_id == users_df.users_id) & (users_df.banned != 'Yes'), 'inner').select(\"id\", \"client_id\", \"driver_id\", \"city_id\", \"status\", \"request_at\")\n",
    "ucd_df = uc_df.join(users_df, (trips_df.driver_id == users_df.users_id) & (users_df.banned != 'Yes'), 'inner').select(\"id\", \"client_id\", \"driver_id\", \"city_id\", \"status\", \"request_at\")\n",
    "ucd_df = ucd_df.withColumn('flg', when(col('status') != 'completed', 1).otherwise(0))\n",
    "cancellation_rates_df = ucd_df.groupBy('request_at').agg( (sum('flg')*100/count('*')).alias('cancelled_trips_rates'))\n",
    "cancellation_rates_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50921d79-d26e-4835-8f7e-c7034f5eff2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Q8. Write an SQL query to find the winner in each group.\n",
    "The winner in each group is the player who scored the maximum total points within the group.\n",
    "In the case of a tie, the lowest player_id wins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0043d6c4-264a-42d1-ab6c-0aadde5c344b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+---+\n|player_id|group_id|score| rn|\n+---------+--------+-----+---+\n|       15|       1|    3|  1|\n|       35|       2|    1|  1|\n|       40|       3|    5|  1|\n+---------+--------+-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "players_data = [\n",
    "    (15, 1),\n",
    "    (25, 1),\n",
    "    (30, 1),\n",
    "    (45, 1),\n",
    "    (10, 2),\n",
    "    (35, 2),\n",
    "    (50, 2),\n",
    "    (20, 3),\n",
    "    (40, 3)\n",
    "]\n",
    "players_columns = [\"player_id\", \"group_id\"]\n",
    "players_df = spark.createDataFrame(players_data, players_columns)\n",
    "matches_data = [\n",
    "    (1, 15, 45, 3, 0),\n",
    "    (2, 30, 25, 1, 2),\n",
    "    (3, 30, 15, 2, 0),\n",
    "    (4, 40, 20, 5, 2),\n",
    "    (5, 35, 50, 1, 1)\n",
    "]\n",
    "from pyspark.sql.functions import col, sum, row_number\n",
    "from pyspark.sql.window import Window\n",
    "matches_columns = [\"match_id\", \"first_player\", \"second_player\", \"first_score\", \"second_score\"]\n",
    "matches_df = spark.createDataFrame(matches_data, matches_columns)\n",
    "first_player_df = matches_df.select(col('first_player').alias('player_id'), col('first_score').alias('score'))\n",
    "second_player_df = matches_df.select(col('second_player').alias('player_id'), col('second_score').alias('score'))\n",
    "player_score_df = first_player_df.union(second_player_df).groupBy('player_id').agg(sum('score').alias('score'))\n",
    "final_df = players_df.join(player_score_df, 'player_id', 'inner')\n",
    "window_spec = Window.partitionBy('group_id').orderBy(col('score').desc(), 'player_id')\n",
    "final_df = final_df.withColumn('rn', row_number().over(window_spec)).filter(col('rn')==1)\n",
    "final_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef72270e-899e-4201-a6ed-8d0ec2face40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Q9. MARKET ANALYSIS: Write an SQL query to find for each seller, whether the brand of the\n",
    "second item (by date) they sold is their favor.\n",
    "If a seller sold less than two items, report the answer for that seller as no.\n",
    "o/p\n",
    "seller id   2nd_item_fav_brand\n",
    "1   yes/no\n",
    "2   yes/no\n",
    "*/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "716c14e6-6501-4fdb-b8e5-b53f1dc69ec1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n|user_id|sold_2nd_fav_item|\n+-------+-----------------+\n|      1|               No|\n|      2|              Yes|\n|      3|              Yes|\n|      4|               No|\n+-------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "users_data = [\n",
    "    (1, '2019-01-01', 'Lenovo'),\n",
    "    (2, '2019-02-09', 'Samsung'),\n",
    "    (3, '2019-01-19', 'LG'),\n",
    "    (4, '2019-05-21', 'HP')\n",
    "]\n",
    "users_columns = [\"user_id\", \"join_date\", \"favorite_brand\"]\n",
    "users_df = spark.createDataFrame(users_data, users_columns)\n",
    "items_data = [\n",
    "    (1, 'Samsung'),\n",
    "    (2, 'Lenovo'),\n",
    "    (3, 'LG'),\n",
    "    (4, 'HP')\n",
    "]\n",
    "items_columns = [\"item_id\", \"item_brand\"]\n",
    "items_df = spark.createDataFrame(items_data, items_columns)\n",
    "orders_data = [\n",
    "    (1, '2019-08-01', 4, 1, 2),\n",
    "    (2, '2019-08-02', 2, 1, 3),\n",
    "    (3, '2019-08-03', 3, 2, 3),\n",
    "    (4, '2019-08-04', 1, 4, 2),\n",
    "    (5, '2019-08-04', 1, 3, 4),\n",
    "    (6, '2019-08-05', 2, 2, 4)\n",
    "]\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import nth_value\n",
    "orders_columns = [\"order_id\", \"order_date\", \"item_id\", \"buyer_id\", \"seller_id\"]\n",
    "orders_df = spark.createDataFrame(orders_data, orders_columns)\n",
    "df = orders_df.join(items_df, 'item_id', 'inner').join(users_df, orders_df.seller_id == users_df.user_id, 'right').select('order_date', 'user_id', 'item_brand', 'favorite_brand' )\n",
    "window_spec = Window.partitionBy('user_id').orderBy('order_date').rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "df = df.withColumn('2nd_sold_item', nth_value('item_brand', 2).over(window_spec))\n",
    "df = df.groupBy('user_id').agg(min('favorite_brand').alias('fav_brand'), min('2nd_sold_item').alias('2nd_sold_item'))\n",
    "final_df = df.select('user_id', when(col('fav_brand')==col('2nd_sold_item'), 'Yes').otherwise('No').alias('sold_2nd_fav_item'))\n",
    "final_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2819f7e-f14b-4625-84df-38777bd39633",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Q10. group the below data into timeframe for continuous same events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a26995c7-4900-4fb7-a829-c0a0c61399f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+-------+\n|grp|start_date|  end_date|  state|\n+---+----------+----------+-------+\n|  0|2019-01-01|2019-01-03|success|\n|  1|2019-01-04|2019-01-05|   fail|\n|  2|2019-01-06|2019-01-06|success|\n+---+----------+----------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tasks_data = [\n",
    "    ('2019-01-01', 'success'),\n",
    "    ('2019-01-02', 'success'),\n",
    "    ('2019-01-03', 'success'),\n",
    "    ('2019-01-04', 'fail'),\n",
    "    ('2019-01-05', 'fail'),\n",
    "    ('2019-01-06', 'success')\n",
    "]\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag, when, to_date, min, max\n",
    "tasks_columns = [\"date_value\", \"state\"]\n",
    "tasks_df = spark.createDataFrame(tasks_data, tasks_columns)\n",
    "tasks_df = tasks_df.withColumn('date_value', to_date(col('date_value'), 'yyyy-MM-dd'))\n",
    "window_spec = Window.orderBy('date_value')\n",
    "tasks_df = tasks_df.withColumn('prev_state', lag(col('state'),1,'success' ).over(window_spec)).withColumn('flg', when(col('state')==col('prev_state'),0).otherwise(1) )\n",
    "tasks_df = tasks_df.withColumn('grp', sum('flg').over(window_spec))\n",
    "tasks_df = tasks_df.groupBy('grp').agg(min('date_value').alias('start_date'), max('date_value').alias('end_date'), min('state').alias('state'))\n",
    "tasks_df.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Ankit Bansal 1-10 questions",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
