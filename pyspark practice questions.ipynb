{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd73413a-0ec2-40a4-b36f-c1b941ed0c00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Q1. different delimiters in a file in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05ca05ac-76a7-46c4-a320-78e0c2a26541",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+-------------+\n| id|   name|age|         city|\n+---+-------+---+-------------+\n|  1|  Alice| 30|     New York|\n|  2|    Bob| 25|  Los Angeles|\n|  3|Charlie| 35|San Francisco|\n|  4|  David| 40|       Boston|\n+---+-------+---+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "# Sample data with multiple string entries and mixed delimiters\n",
    "data = [\n",
    "    \"1,Alice\\t30|New York\",     \n",
    "    \"2|Bob,25\\tLos Angeles\",    \n",
    "    \"3\\tCharlie|35,San Francisco\", \n",
    "    \"4|David,40|Boston\"          \n",
    "]\n",
    "# Creating a DataFrame with a single column\n",
    "df = spark.createDataFrame(data, 'string')\n",
    "split_cols = split(df['value'], ',|\\t|\\|')\n",
    "df = df.withColumn('id', split_cols.getItem(0)).withColumn('name', split_cols.getItem(1)).withColumn('age', split_cols.getItem(2)).withColumn('city', split_cols.getItem(3))\n",
    "\n",
    "df.select('id', 'name', 'age', 'city').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "082577c8-fb1a-424a-b421-0c95f40cabad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Q2. Missing Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31d84aeb-b5e1-43d3-a5d7-9e4af155238a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n|number|\n+------+\n|     3|\n|     9|\n+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "data = [1, 2, 4, 5, 6, 7, 8, 10]\n",
    "df_1 = spark.createDataFrame([(x,) for x in data], [\"number\"])\n",
    "# Generating a complete sequence DataFrame\n",
    "df_2 = spark.range(1,11).toDF(\"number\")\n",
    "\n",
    "missing_numbers = df_2.join(df_1, 'number', 'left_anti')\n",
    "missing_numbers.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99c80527-4e0a-4db6-b8ca-34b19edaff57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "You're right! In the previous example with strings that contained mixed delimiters, we didnâ€™t need to convert each string into a tuple to create a DataFrame. This is because PySpark can directly create a DataFrame from a list of strings, treating each string as a single row in a DataFrame. The reason is that each string already represents a single value, so PySpark treats it as a single column entry by default.\n",
    "\n",
    "For the case with numbers, however, converting to a tuple is necessary because createDataFrame expects rows to be tuples when the list contains individual numbers instead of strings. Unlike strings, numbers do not carry an implicit structure for PySpark to infer a single column. By converting each number to a single-element tuple, we give it a structure that PySpark recognizes as a single column.\n",
    "Summary\n",
    "Strings: When creating a DataFrame from a list of strings, each string is treated as a single row, so no tuple conversion is needed.\n",
    "Numbers: When creating a DataFrame from a list of individual numbers, we need to convert each number to a tuple, as PySpark expects rows to have some structure (like a tuple) for non-string data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5ac394a-4a75-42bf-bbcb-498485e530f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Q3. Top 3 movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76d0a94f-2aa4-4748-aafb-3b3f9e83af17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------+\n|MovieID|MovieName|avg_rating|\n+-------+---------+----------+\n|      1|  Movie A|      4.25|\n|      2|  Movie B|      4.25|\n|      3|  Movie C|       4.0|\n+-------+---------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Sample DataFrames\n",
    "from pyspark.sql.functions import avg, col\n",
    "data_movies = [(1, \"Movie A\"), (2, \"Movie B\"), (3, \"Movie C\"), (4, \"Movie D\"), (5, \"Movie E\")]\n",
    "\n",
    "data_ratings = [(1, 101, 4.5), (1, 102, 4.0), (2, 103, 5.0), \n",
    "                (2, 104, 3.5), (3, 105, 4.0), (3, 106, 4.0), \n",
    "                (4, 107, 3.0), (5, 108, 2.5), (5, 109, 3.0)]\n",
    "\n",
    "columns_movies = [\"MovieID\", \"MovieName\"]\n",
    "columns_ratings = [\"MovieID\", \"UserID\", \"Rating\"]\n",
    "\n",
    "movies_df = spark.createDataFrame(data_movies, columns_movies)\n",
    "ratings_df = spark.createDataFrame(data_ratings, columns_ratings)\n",
    "top_movies = movies_df.join(ratings_df, 'MovieID', 'inner')\n",
    "top_movies = top_movies.groupBy('MovieID', 'MovieName').agg(avg('rating').alias(\"avg_rating\")).orderBy('avg_rating', ascending=False).limit(3)\n",
    "top_movies.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddd10c8b-6fbc-4aca-b949-598b9b686b3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Q4. rolling average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b4b1fde-cea0-42b2-b569-4c9ba8f724de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------------+-----------+\n|      date|productid|quantitysold|rolling_avg|\n+----------+---------+------------+-----------+\n|2023-01-01|      100|          10|       10.0|\n|2023-01-02|      100|          15|       12.5|\n|2023-01-02|      100|          20|       15.0|\n|2023-01-04|      100|          25|       17.5|\n|2023-01-04|      100|          30|       20.0|\n|2023-01-06|      100|          35|       22.5|\n|2023-01-07|      100|          40|       25.0|\n|2023-01-08|      100|          45|       27.5|\n|2023-01-09|      100|          50|       30.0|\n+----------+---------+------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import avg, to_date\n",
    "data = [('2023-01-01', 100, 10),\n",
    "        ('2023-01-02', 100, 15),\n",
    "        ('2023-01-02', 100, 20),\n",
    "        ('2023-01-04', 100, 25),\n",
    "        ('2023-01-04', 100, 30),\n",
    "        ('2023-01-06', 100, 35),\n",
    "        ('2023-01-07', 100, 40),\n",
    "        ('2023-01-08', 100, 45),\n",
    "        ('2023-01-09', 100, 50)\n",
    "        ]\n",
    "columns = ['date', 'productid', 'quantitysold']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df = df.withColumn('date', to_date(col('date'), 'yyyy-MM-dd'))\n",
    "window_col = Window.partitionBy('productid').orderBy('date').rowsBetween(Window.unboundedPreceding,Window.currentRow)\n",
    "df = df.withColumn('rolling_avg', avg('quantitysold').over(window_col))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9d3c78a-9a98-4fe5-84c9-f605c4b115bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Q5. udf function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed0d9728-4c2a-46a6-839b-ef9822c29b4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+--------------+\n|user_id|age|      category|\n+-------+---+--------------+\n|   4001| 17|         young|\n|   4002| 45|         adult|\n|   4003| 65|senior citizen|\n|   4004| 30|         adult|\n|   4005| 80|senior citizen|\n+-------+---+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "data = [(4001, 17), (4002, 45), (4003, 65),(4004, 30), (4005, 80)]\n",
    "columns = ['user_id', 'age']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "def age_categorisation(n):\n",
    "    if n<25:\n",
    "        return 'young'\n",
    "    elif n<60:\n",
    "        return 'adult'\n",
    "    else:\n",
    "        return 'senior citizen'\n",
    "age_udf = udf(age_categorisation, StringType())\n",
    "df = df.withColumn('category', age_udf(col('age')))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f9ecee8-4852-453d-9d49-d8141b9faa22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Q6. Find the count of unique visitors to a website per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7b59911-3e8e-46ec-ae34-33a786f8d7cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+\n|      Date|unique visitors|\n+----------+---------------+\n|2023-01-01|              2|\n|2023-01-02|              2|\n+----------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import to_date, countDistinct\n",
    "\n",
    "visitor_data = [Row(Date='2023-01-01', VisitorID=101),\n",
    "                Row(Date='2023-01-01', VisitorID=102),\n",
    "                Row(Date='2023-01-01', VisitorID=101),\n",
    "                Row(Date='2023-01-02', VisitorID=103),\n",
    "                Row(Date='2023-01-02', VisitorID=101)]\n",
    "\n",
    "# Create DataFrame\n",
    "df_visitors = spark.createDataFrame(visitor_data)\n",
    "df_visitors = df_visitors.withColumn('Date', to_date(col('Date'), 'yyyy-MM-dd'))\n",
    "df_visitors = df_visitors.groupBy('Date').agg(countDistinct('VisitorID').alias('unique visitors'))\n",
    "df_visitors.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5fcdab4-c4d0-4091-aabf-603849a7add9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Q7. Determine the first purchase date for each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bbdb0e6-4060-4758-8a0f-7120c6ffb43a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+\n|UserId|FirstPurchaseDate|\n+------+-----------------+\n|     1|       2023-01-05|\n|     2|       2023-01-03|\n|     3|       2023-01-12|\n+------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min\n",
    "purchase_data = [\n",
    "    Row(UserID=1, PurchaseDate='2023-01-05'),\n",
    "    Row(UserID=1, PurchaseDate='2023-01-10'),\n",
    "    Row(UserID=2, PurchaseDate='2023-01-03'),\n",
    "    Row(UserID=3, PurchaseDate='2023-01-12')\n",
    "]\n",
    "\n",
    "df_purchases = spark.createDataFrame(purchase_data)\n",
    "df_purchases = df_purchases.withColumn(\"PurchaseDate\", to_date(col('PurchaseDate'), 'yyyy-MM-dd'))\n",
    "df_purchases = df_purchases.groupBy('UserId').agg(min('PurchaseDate').alias('FirstPurchaseDate'))\n",
    "df_purchases.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34b19b96-5cd9-46d6-9564-1fdd5f881fde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Q8. Generate a sequential number for each row within each group, ordered by date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1a43398-75dc-41ec-bedc-57cc6c183850",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---+\n|GroupID|      Date| rn|\n+-------+----------+---+\n|      A|2023-01-01|  1|\n|      A|2023-01-02|  2|\n|      B|2023-01-01|  1|\n|      B|2023-01-03|  2|\n+-------+----------+---+\n\n"
     ]
    }
   ],
   "source": [
    "group_data = [\n",
    "    Row(GroupID='A', Date='2023-01-01'),\n",
    "    Row(GroupID='A', Date='2023-01-02'),\n",
    "    Row(GroupID='B', Date='2023-01-01'),\n",
    "    Row(GroupID='B', Date='2023-01-03')\n",
    "]\n",
    "df_group = spark.createDataFrame(group_data)\n",
    "df_group = df_group.withColumn('Date', col('Date').cast('date'))\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "window_spec = Window.partitionBy('GroupId').orderBy('Date')\n",
    "df_group = df_group.withColumn('rn', row_number().over(window_spec))\n",
    "df_group.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcbf7cac-2701-4bc2-a7f6-1f3e5891459e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Q9. Problem: Given a dataset of sales records, identify and replace all missing values in the 'amount' column with the average sales amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83bf5089-2ca9-4183-bd22-84bd2ceeee24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n|sale_id|amount|\n+-------+------+\n|      1|   100|\n|      2|   150|\n|      3|   150|\n|      4|   200|\n|      5|   150|\n+-------+------+\n\n"
     ]
    }
   ],
   "source": [
    "sales_data = [(\"1\", 100), (\"2\", 150), (\"3\", None), (\"4\", 200), (\"5\", None)]\n",
    "sales_df = spark.createDataFrame(sales_data, [\"sale_id\", \"amount\"])\n",
    "from pyspark.sql.functions import mean\n",
    "avg_amount = sales_df.na.drop().agg(mean(col('amount'))).first()[0]\n",
    "sales_df_filled = sales_df.na.fill(avg_amount)\n",
    "sales_df_filled.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96f50910-0705-42e7-89f8-7e19ae99e9b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Q10. unpivoting \n",
    "Problem: Given a dataset of sales records with monthly sales per product, reshape the data to have one row per product-month combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91bd784d-79b4-4fc7-964a-db22fef88033",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------+\n| product|Month|Amount|\n+--------+-----+------+\n|Product1|  Jan|   100|\n|Product1|  Feb|   150|\n|Product1|  Mar|   200|\n|Product2|  Jan|   200|\n|Product2|  Feb|   250|\n|Product2|  Mar|   300|\n|Product3|  Jan|   300|\n|Product3|  Feb|   350|\n|Product3|  Mar|   400|\n+--------+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [(\"Product1\", 100, 150, 200),\n",
    "        (\"Product2\", 200, 250, 300),\n",
    "        (\"Product3\", 300, 350, 400)]\n",
    "columns = [\"Product\", \"Sales_Jan\", \"Sales_Feb\", \"Sales_Mar\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "from pyspark.sql.functions import expr\n",
    "df = df.select('product', expr(\"stack(3, 'Jan', Sales_Jan, 'Feb', Sales_Feb, 'Mar', Sales_Mar) as (Month, Amount)\"))\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d49f9d99-8885-4807-a299-6747f02bd7cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Q11 . \n",
    "you are given a dataframe of students names students_df. write a function named grade_colors to select only rows \n",
    "where student favourute color is either green or red and grade above 90\n",
    "student df == name, age, favourite_clolor, grade "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b1c0d3c-7387-46cc-9295-38b10c2ee908",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------------+-----+\n|name|age|favourite_clolor|grade|\n+----+---+----------------+-----+\n|   a| 19|             red|   91|\n|   e| 23|           green|   93|\n+----+---+----------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "data = [(\"a\", 19, 'red', 91), (\"b\", 20, 'yellow', 95), (\"c\", 21, 'green', 82), (\"d\", 20, 'blue', 75), (\"e\", 23, 'green', 93)]\n",
    "columns = [ \"name\", \"age\", \"favourite_clolor\", \"grade\" ]\n",
    "students_df = spark.createDataFrame(data, columns)\n",
    "def grade_colr(df):\n",
    "    return df.filter((col(\"favourite_clolor\").isin('green', 'red')) & (col('grade')>90))\n",
    "result_df = grade_colr(students_df)\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4fd9706-b50a-4c49-abb3-5890bd881eb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Q.12 Explode with split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f2b753a-82ff-4cd9-8bcb-042c40a7ad87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+\n|empId|  name| location|\n+-----+------+---------+\n|    1|Gaurav|     Pune|\n|    1|Gaurav|Hyderabad|\n|    1|Rishab|     Pune|\n|    1|Rishab|   Mumbai|\n+-----+------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "columns = ['empId', 'name', 'Locations']\n",
    "data = [('1','Gaurav', 'Pune,Bangalore,Hyderabad'), ('1','Rishab', 'Pune,Bangalore,Mumbai')]\n",
    "employee_df = spark.createDataFrame(data, columns)\n",
    "split_df = employee_df.withColumn('location', explode(split(col('Locations'),',')))\n",
    "final_df = split_df.select('empId','name',\"location\").filter(col('location')!='Bangalore')\n",
    "final_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a22814db-cfc1-4d14-8c95-ead4c7a86ba7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Q.13 count the number of movies in each genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "815aea3f-7b65-4c60-a681-0511d1d5823c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n|   genre|count|\n+--------+-----+\n|   Crime|    4|\n|   Drama|    4|\n|Thriller|    2|\n|  Action|    1|\n+--------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "data =[('The Shawshank Redemption',['Drama', 'Crime']),\n",
    "                  ('The Godfather', ['Drama', 'Crime']),\n",
    "                  ('Pulp Fiction', ['Drama', 'Crime','Thriller']),\n",
    "                  ('The Dark Knight', ['Drama', 'Crime','Thriller','Action'])\n",
    "                  ]\n",
    "\n",
    "columns = [\"name\", \"genres\"]\n",
    "movies = spark.createDataFrame(data, columns)\n",
    "df = movies.select('name', explode((col('genres'))).alias('genre'))\n",
    "df = df.groupBy('genre').count()\n",
    "df.show()\n",
    "                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6b0ebae-8032-405f-80f6-a39b29eeed1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Q14. \n",
    "write a pyspark query to repeat a number one times it self\n",
    "eg : input : 1,2,3,5\n",
    "output : 1,2,2,3,3,3,5,5,5,5,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cfeca17-e9f2-4f42-a14a-3503d14cc760",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n|repeated_number|\n+---------------+\n|              1|\n|              2|\n|              2|\n|              3|\n|              3|\n|              3|\n|              5|\n|              5|\n|              5|\n|              5|\n|              5|\n+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import col, expr\n",
    "data = [(1,), (2,), (3,), (5,)]\n",
    "columns = [\"number\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "max_number = df.selectExpr(\"max(number) as max\").first()[\"max\"]\n",
    "sequence_df = spark.range(1, max_number + 1).selectExpr(\"id as seq\")\n",
    "df_repeated = df.crossJoin(sequence_df).filter(col(\"seq\") <= col(\"number\"))\n",
    "df_result = df_repeated.select(col(\"number\").alias(\"repeated_number\"))\n",
    "df_result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cda37a7-b7c6-4bc9-babd-a62bc3203e0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n|number|\n+------+\n|     1|\n|     2|\n|     2|\n|     3|\n|     3|\n|     3|\n|     5|\n|     5|\n|     5|\n|     5|\n|     5|\n+------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql.functions import expr, col\n",
    "data = [(1,), (2,), (3,), (5,)]\n",
    "columns = [\"number\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df_casted = df.withColumn(\"number\", col(\"number\").cast(\"int\"))\n",
    "df_repeated = df_casted.selectExpr(\"explode(array_repeat(number, number)) as number\")\n",
    "df_repeated.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a099c0d-830e-4ded-8f6c-0f86ee411c36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+----------+-------+----------+------------+\n|emp_id|   emp_name|emp_gender|emp_age|emp_salary| emp_manager|\n+------+-----------+----------+-------+----------+------------+\n|     1|Arjun Patel|      Male|     30|     60000|Aarav Sharma|\n|     3| Zara Singh|    Female|     35|     70000| Arjun Patel|\n|     4|Priya Reddy|    Female|     32|     65000|Aarav Sharma|\n+------+-----------+----------+-------+----------+------------+\n\n+------+-----------+----------+-------+----------+------------+\n|emp_id|   emp_name|emp_gender|emp_age|emp_salary| emp_manager|\n+------+-----------+----------+-------+----------+------------+\n|     1|Arjun Patel|      Male|     30|     60000|Aarav Sharma|\n|     1|Arjun Patel|      Male|     30|     60000|Aarav Sharma|\n|     3| Zara Singh|    Female|     35|     70000| Arjun Patel|\n|     4|Priya Reddy|    Female|     32|     65000|Aarav Sharma|\n+------+-----------+----------+-------+----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# finding duplicates in dataframe\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "schema = StructType([\n",
    "    StructField(\"emp_id\", IntegerType(), True),\n",
    "    StructField(\"emp_name\", StringType(), True),\n",
    "    StructField(\"emp_gender\", StringType(), True),\n",
    "    StructField(\"emp_age\", IntegerType(), True),\n",
    "    StructField(\"emp_salary\", IntegerType(), True),\n",
    "    StructField(\"emp_manager\", StringType(), True)\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (1, \"Arjun Patel\", \"Male\", 30, 60000, \"Aarav Sharma\"),\n",
    "    (2, \"Aarav Sharma\", \"Male\", 28, 55000, \"Zara Singh\"),\n",
    "    (3, \"Zara Singh\", \"Female\", 35, 70000, \"Arjun Patel\"),\n",
    "    (4, \"Priya Reddy\", \"Female\", 32, 65000, \"Aarav Sharma\"),\n",
    "    (1, \"Arjun Patel\", \"Male\", 30, 60000, \"Aarav Sharma\"),\n",
    "    (6, \"Naina Verma\", \"Female\", 31, 72000, \"Arjun Patel\"),\n",
    "    (1, \"Arjun Patel\", \"Male\", 30, 60000, \"Aarav Sharma\"),\n",
    "    (4, \"Priya Reddy\", \"Female\", 32, 65000, \"Aarav Sharma\"),\n",
    "    (5, \"Aditya Kapoor\", \"Male\", 28, 58000, \"Zara Singh\"),\n",
    "    (10, \"Anaya Joshi\", \"Female\", 27, 59000, \"Aarav Sharma\"),\n",
    "    (11, \"Rohan Malhotra\", \"Male\", 36, 73000, \"Zara Singh\"),\n",
    "    (3, \"Zara Singh\", \"Female\", 35, 70000, \"Arjun Patel\")\n",
    "]\n",
    "# using groupby\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df = df.groupBy(df.columns).count()\n",
    "df = df.filter(col('count')>1).drop(col('count'))\n",
    "df.show()\n",
    "\n",
    "# using window function\n",
    "\n",
    "df_2 = spark.createDataFrame(data, schema=schema)\n",
    "window_spec = Window.partitionBy(df_2.columns).orderBy(df_2.columns)\n",
    "df_2 = df_2.withColumn('rn', row_number().over(window_spec))\n",
    "df_2 = df_2.filter(col('rn')>1).drop(col('rn'))\n",
    "df_2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a0478b3-93e5-43e2-ab6a-5ac09a63e9ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------+-------+----------+------------+\n|emp_id|      emp_name|emp_gender|emp_age|emp_salary| emp_manager|\n+------+--------------+----------+-------+----------+------------+\n|     1|   Arjun Patel|      Male|     30|     60000|Aarav Sharma|\n|     3|    Zara Singh|    Female|     35|     70000| Arjun Patel|\n|     2|  Aarav Sharma|      Male|     28|     55000|  Zara Singh|\n|     4|   Priya Reddy|    Female|     32|     65000|Aarav Sharma|\n|     6|   Naina Verma|    Female|     31|     72000| Arjun Patel|\n|     5| Aditya Kapoor|      Male|     28|     58000|  Zara Singh|\n|    10|   Anaya Joshi|    Female|     27|     59000|Aarav Sharma|\n|    11|Rohan Malhotra|      Male|     36|     73000|  Zara Singh|\n+------+--------------+----------+-------+----------+------------+\n\n+------+--------------+----------+-------+----------+------------+\n|emp_id|      emp_name|emp_gender|emp_age|emp_salary| emp_manager|\n+------+--------------+----------+-------+----------+------------+\n|     1|   Arjun Patel|      Male|     30|     60000|Aarav Sharma|\n|     2|  Aarav Sharma|      Male|     28|     55000|  Zara Singh|\n|     3|    Zara Singh|    Female|     35|     70000| Arjun Patel|\n|     4|   Priya Reddy|    Female|     32|     65000|Aarav Sharma|\n|     6|   Naina Verma|    Female|     31|     72000| Arjun Patel|\n|     5| Aditya Kapoor|      Male|     28|     58000|  Zara Singh|\n|    10|   Anaya Joshi|    Female|     27|     59000|Aarav Sharma|\n|    11|Rohan Malhotra|      Male|     36|     73000|  Zara Singh|\n+------+--------------+----------+-------+----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# dropping duplicates in \n",
    "\n",
    "# using drop duplicates\n",
    "df_3 = spark.createDataFrame(data, schema=schema)\n",
    "df_3 = df_3.dropDuplicates()\n",
    "df_3.show()\n",
    "\n",
    "# using groupby\n",
    "df_4 = spark.createDataFrame(data, schema=schema)\n",
    "df_4 = df_4.groupBy(df_4.columns).count()\n",
    "df_4 = df_4.drop(col('count'))\n",
    "df_4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4a2e433-dae1-4ff4-bf12-92b4420aef97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pyspark practice questions",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
